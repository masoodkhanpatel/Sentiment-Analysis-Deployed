{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SageMaker Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "conda_pytorch_p36",
      "language": "python",
      "name": "conda_pytorch_p36"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "jxOkQiB9fTYa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating a Sentiment Analysis Web App\n",
        "## Using PyTorch and SageMaker"
      ]
    },
    {
      "metadata": {
        "id": "DgsMLrR0fTYj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading the data\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dmNoO9uCfTYo",
        "colab_type": "code",
        "colab": {},
        "outputId": "fc4d1431-b41a-4767-a15e-6e86ec345e8d"
      },
      "cell_type": "code",
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘../data’: File exists\n",
            "--2019-03-08 19:31:42--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  25.8MB/s    in 3.1s    \n",
            "\n",
            "2019-03-08 19:31:46 (25.8 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cbkf2DINfTZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Preparing and Processing the data\n"
      ]
    },
    {
      "metadata": {
        "id": "WOg4E9KEfTZF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "    \n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "        \n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "            \n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "            \n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "                    \n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "                \n",
        "    return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4g0ZYYlYfTZS",
        "colab_type": "code",
        "colab": {},
        "outputId": "391780ba-1ccf-4862-865d-f63bebd3e092"
      },
      "cell_type": "code",
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W8uRw8hJfTZk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "    \n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "    \n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "    \n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z29oDLPCfTZs",
        "colab_type": "code",
        "colab": {},
        "outputId": "73d5cc6c-cd2a-437c-ee5d-63c2aefb0e27"
      },
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a2aGDiJDfTaD",
        "colab_type": "code",
        "colab": {},
        "outputId": "c03e4ad4-5994-4cd3-954d-a12922bae75e"
      },
      "cell_type": "code",
      "source": [
        "print(train_X[100])\n",
        "print(train_y[100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One of my favorite movies which has been overlooked by too many movie goers, an observation which mystifies me. Not only directed by the acclaimed Ang Lee,it had many young actors who were to become major stars, e.g., Tobey Maguire (before Spiderman), Skeet Ulrich (before Jericho), Jonathan Rhys Meyers (before Tudors), James Caviezel, Simon Baker, Mark Ruffalo, Jeffrey Wright, Tom Wilkinson, and Jewel. All of the acting was superb and each of the actors mentioned gave memorable performances, especially Meyers who portrayed an evil villain who killed for the sake of killing.<br /><br />When the biographies and accomplishments of the director ( even when he won an academy award) and the actors are listed, this film is usually omitted from their past performances. I discovered the film on DVD by accident and it became one of my most often watched films. However, it is seldom every seen on cable. I look forward to reading what others suggest are the reasons this film is not well known.\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UJ-HuL-lfTai",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "    \n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "    \n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T2Gl_8safTay",
        "colab_type": "code",
        "colab": {},
        "outputId": "0d95eae1-53e1-4c89-c283-47120f28feee"
      },
      "cell_type": "code",
      "source": [
        "review_to_words(train_X[100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one',\n",
              " 'favorit',\n",
              " 'movi',\n",
              " 'overlook',\n",
              " 'mani',\n",
              " 'movi',\n",
              " 'goer',\n",
              " 'observ',\n",
              " 'mystifi',\n",
              " 'direct',\n",
              " 'acclaim',\n",
              " 'ang',\n",
              " 'lee',\n",
              " 'mani',\n",
              " 'young',\n",
              " 'actor',\n",
              " 'becom',\n",
              " 'major',\n",
              " 'star',\n",
              " 'e',\n",
              " 'g',\n",
              " 'tobey',\n",
              " 'maguir',\n",
              " 'spiderman',\n",
              " 'skeet',\n",
              " 'ulrich',\n",
              " 'jericho',\n",
              " 'jonathan',\n",
              " 'rhi',\n",
              " 'meyer',\n",
              " 'tudor',\n",
              " 'jame',\n",
              " 'caviezel',\n",
              " 'simon',\n",
              " 'baker',\n",
              " 'mark',\n",
              " 'ruffalo',\n",
              " 'jeffrey',\n",
              " 'wright',\n",
              " 'tom',\n",
              " 'wilkinson',\n",
              " 'jewel',\n",
              " 'act',\n",
              " 'superb',\n",
              " 'actor',\n",
              " 'mention',\n",
              " 'gave',\n",
              " 'memor',\n",
              " 'perform',\n",
              " 'especi',\n",
              " 'meyer',\n",
              " 'portray',\n",
              " 'evil',\n",
              " 'villain',\n",
              " 'kill',\n",
              " 'sake',\n",
              " 'kill',\n",
              " 'biographi',\n",
              " 'accomplish',\n",
              " 'director',\n",
              " 'even',\n",
              " 'academi',\n",
              " 'award',\n",
              " 'actor',\n",
              " 'list',\n",
              " 'film',\n",
              " 'usual',\n",
              " 'omit',\n",
              " 'past',\n",
              " 'perform',\n",
              " 'discov',\n",
              " 'film',\n",
              " 'dvd',\n",
              " 'accid',\n",
              " 'becam',\n",
              " 'one',\n",
              " 'often',\n",
              " 'watch',\n",
              " 'film',\n",
              " 'howev',\n",
              " 'seldom',\n",
              " 'everi',\n",
              " 'seen',\n",
              " 'cabl',\n",
              " 'look',\n",
              " 'forward',\n",
              " 'read',\n",
              " 'other',\n",
              " 'suggest',\n",
              " 'reason',\n",
              " 'film',\n",
              " 'well',\n",
              " 'known']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "gFbKf5UTfTbG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "    \n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "        \n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "    \n",
        "    return words_train, words_test, labels_train, labels_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEMD75uIfTbU",
        "colab_type": "code",
        "colab": {},
        "outputId": "ef0b03e0-6310-4c54-8c6a-85aebeac619e"
      },
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read preprocessed data from cache file: preprocessed_data.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OfbazOJhfTbh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Transform the data"
      ]
    },
    {
      "metadata": {
        "id": "HyMCyMJ_fTbk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a word dictionary"
      ]
    },
    {
      "metadata": {
        "id": "engXHfe0fTbm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def build_dict(data, vocab_size = 5000):\n",
        "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
        "    \n",
        "    # Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
        "    #       sentence is a list of words.\n",
        "    \n",
        "    word_count = Counter([each for sentence in data for each in sentence]) # A dict storing the words that appear in the reviews along with how often they occur\n",
        "    \n",
        "    # Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
        "    #       sorted_words[-1] is the least frequently appearing word.\n",
        "    \n",
        "    sorted_words = sorted(word_count, key=word_count.get, reverse=True)\n",
        "    \n",
        "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
        "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
        "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
        "        \n",
        "    return word_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "__0y21YKfTbu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_dict = build_dict(train_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YkQliWDxfTcI",
        "colab_type": "code",
        "colab": {},
        "outputId": "4d7d2620-81ca-498d-b27e-e8e57b70d020"
      },
      "cell_type": "code",
      "source": [
        "# five most frequently appearing words in the training set.\n",
        "list(word_dict.keys())[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['movi', 'film', 'one', 'like', 'time']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "K-v8kNGHfTcZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Save `word_dict`"
      ]
    },
    {
      "metadata": {
        "id": "b9NHvwL1fTcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
        "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
        "    os.makedirs(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-UhgTTvUfTcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
        "    pickle.dump(word_dict, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uc4GpqIJfTcw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transform the reviews"
      ]
    },
    {
      "metadata": {
        "id": "VJnQ1mdsfTc1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_and_pad(word_dict, sentence, pad=500):\n",
        "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
        "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
        "    \n",
        "    working_sentence = [NOWORD] * pad\n",
        "    \n",
        "    for word_index, word in enumerate(sentence[:pad]):\n",
        "        if word in word_dict:\n",
        "            working_sentence[word_index] = word_dict[word]\n",
        "        else:\n",
        "            working_sentence[word_index] = INFREQ\n",
        "            \n",
        "    return working_sentence, min(len(sentence), pad)\n",
        "\n",
        "def convert_and_pad_data(word_dict, data, pad=500):\n",
        "    result = []\n",
        "    lengths = []\n",
        "    \n",
        "    for sentence in data:\n",
        "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
        "        result.append(converted)\n",
        "        lengths.append(leng)\n",
        "        \n",
        "    return np.array(result), np.array(lengths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dV7PiGp3fTdD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
        "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YCKCKDhBfTdM",
        "colab_type": "code",
        "colab": {},
        "outputId": "a06b0c9a-5b28-4f18-c9bb-59415b2e1eef"
      },
      "cell_type": "code",
      "source": [
        "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
        "x = [each for each in train_X[100] if not each==0]\n",
        "y = [each for each in train_X[100] if each==0]\n",
        "print('Post processed review: ',x)\n",
        "print('Non-zeros(words): ', len(x),' Remaining 0\\'s(empty): ', len(y))\n",
        "print(len(train_X[100]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Post processed review:  [26, 2, 14, 74, 1024, 23, 260, 26, 132, 155, 70, 1339, 14, 77, 42, 576, 322, 305, 1543, 776, 44, 72, 39, 77, 29, 63, 1, 2373, 5, 5, 2007, 465, 907, 3487, 10, 1, 2373, 1144, 26, 2, 14, 99, 23, 1, 3487, 25, 11, 1684, 77, 16, 77, 132, 23, 1065, 2, 14, 47, 1684, 323, 6, 57, 2, 352, 456, 26, 193, 519, 107, 138, 59, 1571, 161, 48, 624, 44, 72, 4480, 12]\n",
            "Non-zeros(words):  78  Remaining 0's(empty):  422\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "21hYyfHRfTdk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Upload the data to S3\n",
        "\n",
        "### Save the processed training dataset locally"
      ]
    },
    {
      "metadata": {
        "id": "IrPT2GHSfTdl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "    \n",
        "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
        "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HnD6_hw5fTdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Uploading the training data\n"
      ]
    },
    {
      "metadata": {
        "id": "3r_ruAPrfTdx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "prefix = 'sagemaker/sentiment_rnn'\n",
        "\n",
        "role = sagemaker.get_execution_role()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkxZ-XKJfTd3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JgLQ0xsEfTeB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build and Train the PyTorch Model\n"
      ]
    },
    {
      "metadata": {
        "id": "XfssdPFmfTeC",
        "colab_type": "code",
        "colab": {},
        "outputId": "7bcb6ffe-b653-4ae8-ecd7-186ced8678f1"
      },
      "cell_type": "code",
      "source": [
        "!pygmentize train/model.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
            "\r\n",
            "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
            "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
            "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
            "\r\n",
            "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
            "        \r\n",
            "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[36mNone\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        x = x.t()\r\n",
            "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
            "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
            "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
            "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
            "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
            "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
            "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ovz_EL5rfTeN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Read in only the first 250 rows\n",
        "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
        "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
        "# Build the dataloader\n",
        "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZxLivsyfTeP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training method"
      ]
    },
    {
      "metadata": {
        "id": "Fa2XzAtafTeS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:         \n",
        "            batch_X, batch_y = batch\n",
        "            \n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            \n",
        "            # TODO: Complete this train method to train the model provided.\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model.forward(batch_X)\n",
        "            # calculate the batch loss\n",
        "            loss = loss_fn(output, batch_y)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.data.item()\n",
        "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LnnLy8D5fTeV",
        "colab_type": "code",
        "colab": {},
        "outputId": "ac778b51-1d2b-42bf-f40f-2ed38f03dbdf"
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from train.model import LSTMClassifier\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMClassifier(32, 100, 5000).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, BCELoss: 0.6919290065765381\n",
            "Epoch: 2, BCELoss: 0.6814518690109252\n",
            "Epoch: 3, BCELoss: 0.6716896176338196\n",
            "Epoch: 4, BCELoss: 0.6600986003875733\n",
            "Epoch: 5, BCELoss: 0.6447454452514648\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U0rQt9EifTel",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ]
    },
    {
      "metadata": {
        "id": "l1KXqiEWfTem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sagemaker.pytorch import PyTorch\n",
        "\n",
        "estimator = PyTorch(entry_point=\"train.py\",\n",
        "                    source_dir=\"train\",\n",
        "                    role=role,\n",
        "                    framework_version='0.4.0',\n",
        "                    train_instance_count=1,\n",
        "                    train_instance_type='ml.m4.4xlarge', # Using a bit more powerful instance AWS SageMaker instance\n",
        "                    hyperparameters={\n",
        "                        'epochs': 10,\n",
        "                        'hidden_dim': 200,\n",
        "                    })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LT0efImfTet",
        "colab_type": "code",
        "colab": {},
        "outputId": "6f3a334b-b415-42c9-ddda-5b84fb11e441"
      },
      "cell_type": "code",
      "source": [
        "estimator.fit({'training': input_data})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Creating training-job with name: sagemaker-pytorch-2019-03-08-19-32-35-134\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-03-08 19:32:35 Starting - Starting the training job...\n",
            "2019-03-08 19:32:37 Starting - Launching requested ML instances......\n",
            "2019-03-08 19:33:39 Starting - Preparing the instances for training...\n",
            "2019-03-08 19:34:33 Downloading - Downloading input data\n",
            "2019-03-08 19:34:33 Training - Downloading the training image..\n",
            "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
            "\u001b[31mbash: no job control in this shell\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:42,248 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:42,250 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:42,264 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:43,671 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:43,885 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
            "\u001b[31mGenerating setup.py\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:43,885 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:43,885 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:43,885 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
            "\u001b[31m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
            "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
            "\u001b[31mCollecting pandas (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/a3/c42cd52e40527ba35aed53a988c485ffeddbae0722b8b756da82464baa73/pandas-0.24.1-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
            "\u001b[31mCollecting numpy (from -r requirements.txt (line 2))\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/18/4f013c3c3051f4e0ffbaa4bf247050d6d5e527fe9cb1907f5975b172f23f/numpy-1.16.2-cp35-cp35m-manylinux1_x86_64.whl (17.2MB)\u001b[0m\n",
            "\u001b[31mCollecting nltk (from -r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\u001b[0m\n",
            "\u001b[31mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/5d/3260694a59df0ec52f8b4883f5d23b130bc237602a1411fa670eae12351e/beautifulsoup4-4.7.1-py3-none-any.whl (94kB)\u001b[0m\n",
            "\u001b[31mCollecting html5lib (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\u001b[0m\n",
            "\u001b[31mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/61/28/1d3920e4d1d50b19bc5d24398a7cd85cc7b9a75a490570d5a30c57622d34/pytz-2018.9-py2.py3-none-any.whl (510kB)\u001b[0m\n",
            "\u001b[31mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
            "\u001b[31mRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (1.11.0)\u001b[0m\n",
            "\u001b[31mCollecting singledispatch (from nltk->-r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\u001b[0m\n",
            "\u001b[31mCollecting soupsieve>=1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/77/78/bca00cc9fa70bba1226ee70a42bf375c4e048fe69066a0d9b5e69bc2a79a/soupsieve-1.8-py2.py3-none-any.whl (88kB)\u001b[0m\n",
            "\u001b[31mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
            "\u001b[31mBuilding wheels for collected packages: nltk, train\n",
            "  Running setup.py bdist_wheel for nltk: started\u001b[0m\n",
            "\u001b[31m  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
            "  Running setup.py bdist_wheel for train: started\n",
            "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rn3d052q/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
            "\u001b[31mSuccessfully built nltk train\u001b[0m\n",
            "\u001b[31mInstalling collected packages: pytz, numpy, pandas, singledispatch, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
            "  Found existing installation: numpy 1.15.4\u001b[0m\n",
            "\u001b[31m    Uninstalling numpy-1.15.4:\n",
            "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
            "\n",
            "2019-03-08 19:34:41 Training - Training image download completed. Training in progress.\u001b[31mSuccessfully installed beautifulsoup4-4.7.1 html5lib-1.0.1 nltk-3.4 numpy-1.16.2 pandas-0.24.1 pytz-2018.9 singledispatch-3.4.0.3 soupsieve-1.8 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
            "\u001b[31mYou are using pip version 18.1, however version 19.0.3 is available.\u001b[0m\n",
            "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:55,056 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
            "\u001b[31m2019-03-08 19:34:55,070 sagemaker-containers INFO     Invoking user script\n",
            "\u001b[0m\n",
            "\u001b[31mTraining Env:\n",
            "\u001b[0m\n",
            "\u001b[31m{\n",
            "    \"log_level\": 20,\n",
            "    \"module_dir\": \"s3://sagemaker-us-east-2-928425406214/sagemaker-pytorch-2019-03-08-19-32-35-134/source/sourcedir.tar.gz\",\n",
            "    \"model_dir\": \"/opt/ml/model\",\n",
            "    \"num_cpus\": 16,\n",
            "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
            "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
            "    \"module_name\": \"train\",\n",
            "    \"hosts\": [\n",
            "        \"algo-1\"\n",
            "    ],\n",
            "    \"current_host\": \"algo-1\",\n",
            "    \"output_dir\": \"/opt/ml/output\",\n",
            "    \"input_dir\": \"/opt/ml/input\",\n",
            "    \"channel_input_dirs\": {\n",
            "        \"training\": \"/opt/ml/input/data/training\"\n",
            "    },\n",
            "    \"resource_config\": {\n",
            "        \"current_host\": \"algo-1\",\n",
            "        \"network_interface_name\": \"ethwe\",\n",
            "        \"hosts\": [\n",
            "            \"algo-1\"\n",
            "        ]\n",
            "    },\n",
            "    \"num_gpus\": 0,\n",
            "    \"job_name\": \"sagemaker-pytorch-2019-03-08-19-32-35-134\",\n",
            "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
            "    \"network_interface_name\": \"ethwe\",\n",
            "    \"input_data_config\": {\n",
            "        \"training\": {\n",
            "            \"TrainingInputMode\": \"File\",\n",
            "            \"S3DistributionType\": \"FullyReplicated\",\n",
            "            \"RecordWrapperType\": \"None\"\n",
            "        }\n",
            "    },\n",
            "    \"hyperparameters\": {\n",
            "        \"epochs\": 10,\n",
            "        \"hidden_dim\": 200\n",
            "    },\n",
            "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
            "    \"additional_framework_parameters\": {},\n",
            "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
            "\u001b[31m}\n",
            "\u001b[0m\n",
            "\u001b[31mEnvironment variables:\n",
            "\u001b[0m\n",
            "\u001b[31mSM_CHANNELS=[\"training\"]\u001b[0m\n",
            "\u001b[31mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
            "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
            "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2019-03-08-19-32-35-134\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-928425406214/sagemaker-pytorch-2019-03-08-19-32-35-134/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"ethwe\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
            "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
            "\u001b[31mSM_NUM_CPUS=16\u001b[0m\n",
            "\u001b[31mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
            "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
            "\u001b[31mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
            "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
            "\u001b[31mSM_HP_EPOCHS=10\u001b[0m\n",
            "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
            "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
            "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
            "\u001b[31mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
            "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
            "\u001b[31mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
            "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
            "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
            "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
            "\u001b[31mSM_MODULE_NAME=train\u001b[0m\n",
            "\u001b[31mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
            "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
            "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-2-928425406214/sagemaker-pytorch-2019-03-08-19-32-35-134/source/sourcedir.tar.gz\u001b[0m\n",
            "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
            "\u001b[31mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
            "\u001b[31mSM_NUM_GPUS=0\n",
            "\u001b[0m\n",
            "\u001b[31mInvoking script with the following command:\n",
            "\u001b[0m\n",
            "\u001b[31m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[31mUsing device cpu.\u001b[0m\n",
            "\u001b[31mGet train data loader.\u001b[0m\n",
            "\u001b[31mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mEpoch: 1, BCELoss: 0.674865647238128\u001b[0m\n",
            "\u001b[31mEpoch: 2, BCELoss: 0.6149366279037631\u001b[0m\n",
            "\u001b[31mEpoch: 3, BCELoss: 0.5503111141068595\u001b[0m\n",
            "\u001b[31mEpoch: 4, BCELoss: 0.4773198414822014\u001b[0m\n",
            "\u001b[31mEpoch: 5, BCELoss: 0.42277174093285386\u001b[0m\n",
            "\u001b[31mEpoch: 6, BCELoss: 0.3790618613058207\u001b[0m\n",
            "\u001b[31mEpoch: 7, BCELoss: 0.3327182774641076\u001b[0m\n",
            "\u001b[31mEpoch: 8, BCELoss: 0.31351238854077396\u001b[0m\n",
            "\u001b[31mEpoch: 9, BCELoss: 0.28995625127334984\u001b[0m\n",
            "\n",
            "2019-03-08 20:18:30 Uploading - Uploading generated training model\n",
            "2019-03-08 20:18:30 Completed - Training job completed\n",
            "\u001b[31mEpoch: 10, BCELoss: 0.2736748508652862\u001b[0m\n",
            "\u001b[31m2019-03-08 20:18:23,002 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
            "Billable seconds: 2645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Bmcm7BOfTe8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing the model\n",
        "\n",
        "## Deploy the model for testing"
      ]
    },
    {
      "metadata": {
        "id": "HIqyODTyfTe_",
        "colab_type": "code",
        "colab": {},
        "outputId": "630c29c7-45d2-47b7-864a-4ba23deb8d58"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Deploy the trained model\n",
        "predictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Creating model with name: sagemaker-pytorch-2019-03-08-19-32-35-134\n",
            "INFO:sagemaker:Creating endpoint with name sagemaker-pytorch-2019-03-08-19-32-35-134\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lOnzmuT2fTfI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use the model for testing"
      ]
    },
    {
      "metadata": {
        "id": "hfw-xC5rfTfK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FSlK63VIfTfN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
        "\n",
        "def predict(data, rows=512):\n",
        "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
        "    predictions = np.array([])\n",
        "    for array in split_array:\n",
        "        predictions = np.append(predictions, predictor.predict(array))\n",
        "    \n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-a0Z56KpfTfR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = predict(test_X.values)\n",
        "predictions = [round(num) for num in predictions]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XpZaEkenfTfX",
        "colab_type": "code",
        "colab": {},
        "outputId": "8e3e51a0-1bfe-4332-bd8b-98ed20e8351b"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test_y, predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "xsOnRnK-fTgA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Delete the endpoint"
      ]
    },
    {
      "metadata": {
        "id": "r4vdEmczfTgB",
        "colab_type": "code",
        "colab": {},
        "outputId": "e80e2ebc-0461-490d-e7d0-f7d6ee27a874"
      },
      "cell_type": "code",
      "source": [
        "estimator.delete_endpoint()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Deleting endpoint with name: sagemaker-pytorch-2019-03-08-19-32-35-134\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "TlVkDqbNfTgP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deploy the model for the web app"
      ]
    },
    {
      "metadata": {
        "id": "OUJn3J9XfTgR",
        "colab_type": "code",
        "colab": {},
        "outputId": "8c1d0870-1de4-4bfe-9854-74f456c9eac3"
      },
      "cell_type": "code",
      "source": [
        "!pygmentize serve/predict.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.nn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.optim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch.utils.data\u001b[39;49;00m\n",
            "\n",
            "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\n",
            "\n",
            "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
            "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "\n",
            "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\n",
            "    model_info = {}\n",
            "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
            "        model_info = torch.load(f)\n",
            "\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: {}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\n",
            "\n",
            "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
            "\n",
            "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\n",
            "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
            "        model.load_state_dict(torch.load(f))\n",
            "\n",
            "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\n",
            "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
            "        model.word_dict = pickle.load(f)\n",
            "\n",
            "    model.to(device).eval()\n",
            "\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    \u001b[34mreturn\u001b[39;49;00m model\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
            "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "        \u001b[34mreturn\u001b[39;49;00m data\n",
            "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\n",
            "\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
            "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
            "    \n",
            "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\n",
            "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
            "    \n",
            "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\n",
            "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\n",
            "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\n",
            "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\n",
            "\n",
            "    data_X ,data_len = convert_and_pad(model.word_dict, review_to_words(input_data))\n",
            "\n",
            "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\n",
            "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\n",
            "    data_pack = np.hstack((data_len, data_X))\n",
            "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\n",
            "    \n",
            "    data = torch.from_numpy(data_pack)\n",
            "    data = data.to(device)\n",
            "\n",
            "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\n",
            "    model.eval()\n",
            "\n",
            "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\n",
            "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\n",
            "\n",
            "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
            "        output = model.forward(data)\n",
            "        \n",
            "    result = np.round(output.numpy())\n",
            "\n",
            "    \u001b[34mreturn\u001b[39;49;00m result\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UmjA8hflfTgb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Deploying the model"
      ]
    },
    {
      "metadata": {
        "id": "NxH9BGLzfTgd",
        "colab_type": "code",
        "colab": {},
        "outputId": "75f6e754-7ce4-4610-97d6-154b6241b02e"
      },
      "cell_type": "code",
      "source": [
        "from sagemaker.predictor import RealTimePredictor\n",
        "from sagemaker.pytorch import PyTorchModel\n",
        "\n",
        "class StringPredictor(RealTimePredictor):\n",
        "    def __init__(self, endpoint_name, sagemaker_session):\n",
        "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
        "\n",
        "model = PyTorchModel(model_data=estimator.model_data,\n",
        "                     role = role,\n",
        "                     framework_version='0.4.0',\n",
        "                     entry_point='predict.py',\n",
        "                     source_dir='serve',\n",
        "                     predictor_cls=StringPredictor)\n",
        "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Creating model with name: sagemaker-pytorch-2019-03-08-20-32-17-532\n",
            "INFO:sagemaker:Creating endpoint with name sagemaker-pytorch-2019-03-08-20-32-17-532\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------------------------------!"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3ou7q4_kfTgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ]
    },
    {
      "metadata": {
        "id": "ZKLr-eBGfTgj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
        "    \n",
        "    results = []\n",
        "    ground = []\n",
        "    \n",
        "    # We make sure to test both positive and negative reviews    \n",
        "    for sentiment in ['pos', 'neg']:\n",
        "        \n",
        "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
        "        files = glob.glob(path)\n",
        "        \n",
        "        files_read = 0\n",
        "        \n",
        "        print('Starting ', sentiment, ' files')\n",
        "        \n",
        "        # Iterate through the files and send them to the predictor\n",
        "        for f in files:\n",
        "            with open(f) as review:\n",
        "                # First, we store the ground truth (was the review positive or negative)\n",
        "                if sentiment == 'pos':\n",
        "                    ground.append(1)\n",
        "                else:\n",
        "                    ground.append(0)\n",
        "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
        "                review_input = review.read().encode('utf-8')\n",
        "                # Send the review to the predictor and store the results\n",
        "                results.append(float(predictor.predict(review_input)))\n",
        "                \n",
        "            # Sending reviews to our endpoint one at a time takes a while so we\n",
        "            # only send a small number of reviews\n",
        "            files_read += 1\n",
        "            if files_read == stop:\n",
        "                break\n",
        "            \n",
        "    return ground, results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eiyR9twIfTgx",
        "colab_type": "code",
        "colab": {},
        "outputId": "6491f9eb-6c70-49c0-c8e4-ce796bf57963"
      },
      "cell_type": "code",
      "source": [
        "ground, results = test_reviews()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting  pos  files\n",
            "Starting  neg  files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "laarAbNJfThE",
        "colab_type": "code",
        "colab": {},
        "outputId": "1a5137a4-e267-4112-9534-c03c374d30c9"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(ground, results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "0DW0ljwMfThX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use the model for the web app\n",
        "\n",
        "### Setup a Lambda function\n",
        "\n",
        "#### Create an IAM Role for the Lambda function\n",
        "\n",
        "#### Create a Lambda function\n",
        "\n",
        "```python\n",
        "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
        "# is not available natively through Lambda.\n",
        "import boto3\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "\n",
        "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
        "    runtime = boto3.Session().client('sagemaker-runtime')\n",
        "\n",
        "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
        "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint we created\n",
        "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
        "                                       Body = event['body'])                       # The actual review\n",
        "\n",
        "    # The response is an HTTP response whose body contains the result of our inference\n",
        "    result = response['Body'].read().decode('utf-8')\n",
        "\n",
        "    return {\n",
        "        'statusCode' : 200,\n",
        "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
        "        'body' : result\n",
        "    }\n",
        "```\n",
        "\n",
        "Once you have copy and pasted the code above into the Lambda code editor, replace the `**ENDPOINT NAME HERE**` portion with the name of the endpoint that we deployed earlier. You can determine the name of the endpoint using the code cell below."
      ]
    },
    {
      "metadata": {
        "id": "_2tzuEcufThZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "9c7f1a4c-69b1-4421-a5b4-47eec0ae8f90"
      },
      "cell_type": "code",
      "source": [
        "# endpoint name\n",
        "predictor.endpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sagemaker-pytorch-2019-03-08-20-32-17-532'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "jxYdbCMGfThh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Setup API Gateway"
      ]
    },
    {
      "metadata": {
        "id": "PLgzmUd6fThi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deploy the web app using"
      ]
    },
    {
      "metadata": {
        "id": "OP2XUCQvfThz",
        "colab_type": "code",
        "colab": {},
        "outputId": "039b29e0-54a2-44d2-e7d4-7792f9236052"
      },
      "cell_type": "code",
      "source": [
        "# Finally deleting the endpoint to stop incurring charges\n",
        "predictor.delete_endpoint()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-pytorch-2019-03-08-20-32-17-532\n",
            "INFO:sagemaker:Deleting endpoint with name: sagemaker-pytorch-2019-03-08-20-32-17-532\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "fguF1eh6fTh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}